---
layout: post
title: Neural Nets basics
subtitle: Visualizing Learning rate vs Batch size
---

This will be a short post but if you are here biggest chances are, you are a relative/friend of mine and/or you want to get insights about neural nets basics.
 
Ok... only one week passed since the beginning of Fastai [http://www.fast.ai/] 1.v2 international course[] but we are already practising with cutting edge tools and concepts. The amount of information and code that flows is so overwhelming that it is hard to choose what to focus on. 

I know myself and if I wait for the dust to settle and everything be tidy in my head I will contribute and post when I am 90 year old. So, my plan, if I think an idea can help anyone that is just a small step behind me I will post it. At least it will be a place for me to keep all those small insights in one place.

So... first lesson focused on being able to classify the famous Kaggle competition dataset "dogs vs. cats" with pretrained convnets. Even if you think "yes, I have done that already", I advise you to watch the video in some months from now, when it becomes public. Because I also "have done that already" but the approaches, tips, and opinions Jeremy Howard gives, as well as the tools are... well, just as intended, cutting edge.  

One of the very remarcables tools gave the posibility to find the optimal learning rate for a given dataset. Being learning rate both extremly important and quite difficult to tune (before this) I found this great news. This is how this tool looks like:

<img src="img/loss_vs_lr.PNG">

There you have it. Loss vs. learning rate. From that its easy to choose an appropiate learning rate.

But this only, impressive as it can be,   would not have motivated me to post on the subject. What I found promising is the possibility of using this tool  to visualize the famous relationship between learning rate and batch size.

For the ones anaware, general rule is "bigger batch size bigger learning rate". This is just logical because bigger batch size means more confidence in the direction of your "descent" of the error surface while the smaller a batch size is the closer you are to "stochastic" descent (batch size 1), that also works but where direction of individual "steps" is more... well, more stochastic. 

Sometimes it is true that a picture is worth one thousand words:

<img src="img/learning_rate_vs_batch_size_g.PNG">


There you have it, the relationship between learning rate error plotted from batches from 64 to 8 for the "cats vs. dogs" dataset. As expected bigger batch size shows bigger optimal learning rate, but if you are like me maybe will find it nice to see this relationship visualized, the differences in the curves as well as the increasing noise of the relationship as batch size decreases.

