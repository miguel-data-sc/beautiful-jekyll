---
layout: post
title: Yes, Convolutional Neural Nets do care about Scale
subtitle: Understanding Fast.ai "progressive resizing" technique. 
---

The arrival of  [publicly available fast.ai deep learning library](https://github.com/fastai/fastai/tree/master/courses/dl1) is great news, for many reasons; cutting edge tools intended to be as convenient as possible. USF  and International Fellowship students are already learning how powerful this new framework is.

Something remarkable is that a good part of the techniques are so new that there’s lots of room for understanding them better. Even if it is empirically demonstrated their efficiency the rationale behind them is something we can still think about. It seems to me that deeper understanding will make us also better practitioners.

In this post I will focus on a technique used in lesson 2 of cited library for the Amazon Planet competition. **The technique consists in the progressive, sequential resizing of all images while training** the convnet, from smaller to bigger sizes.

Obvious? Think twice. Why should this work better than choosing an only optimal size?  What makes this different from zoomed images product of image augmentation? Ok, the difficult one is the former question but we will try to answer both, so let’s uncover what this post will be about:

This post will attempt to understand what scale means to a convolutional network. “Scale invariance” of learnt patterns, if we are going to be precise. I have talked to enough ML practitioners to know that this is not a completely grasped topic, most of us are just happy repeating to ourselves that CNNs are resilient to scale changes, that they learn patterns even if scale changes… but **do we really understand how, or to what extent scale "doesn't matter"?** 
Digging a bit deeper on this will help us also in understanding why cited fast.ai “progressive re-scaling” technique can actually work.

**Warning:** This post, even if not including heavy math formulas is intended to be slightly advanced at an abstract level. I assume some familiarity with what a CNN is, how kernels work, what pooling does… and have thought a bit about all this. If not I recommend you stop reading now, go through the basics  on Convolutional Neural Networks and then caome back if you suddenly find yourself wondering about how size of things influence performance in CNNs.

So, what is this “scale invariance” about? I will introduce the topic by answering some simple questions. Make sure to think about the answer to each of them:

Suppose you  train a CNN with images containing this kitten face, this size and only this size, in different parts of the image, like this…


<img src="/img/imagenes1.PNG" height="350" width="850"> 

Now, prediction time, you have this image:

<img src="/img/imagenes2.PNG" height="350" width="350"> 

What do you think? Will the network be able to detect the zoomed cat? What about the opposite?

Especifically, think about  this two possible patterns that could be learnt to identify cats:


Please, notice that this patterns are essentially different. The ear is self similar, something typical of fractals (you can cut the ear and the top of it still contains an ear, the “earnes” of that pattern is pressent as long as the first few pixels of the tip are present). But the eyes are not self similar. If you scale the eyes the pattern black pixel- empty pixel – black pixel is lost, it is not present in the scaled, bigger picture. If this pattern was learnt in the small version it will not be detected in  a bigger version. This pattern is not scale invariant. What is the first important intuition about this? 

<img src="/img/imagenes3.PNG" height="350" width="350"> 


HINT NUMBER ONE: scale invariance is a property of (some) of the features, not of the network. The implications fo this are that recognition of a pattern that is not scale invariant can only be made at the same scale it was learn. One could wrongly assume of this the necessity of exacly matching size  of images for recognition of this non invariant patterns… but reality is nicer to us: enter pooling concept.

It is know that max-pooling has many virtues. The main one is that it makes a convnet be able to detect a previously learnt pattern in a region bigger than the original, specific location of that pattern. Its quite obvious that compounding of pooling layers gives a CNN the capacity to learn patterns with resilience to  traslation; no matter where the pattern appears it will be detected. Less obvious thought, is how this also means scale variation resilience. But this is also true for self similar patterns: As long as a pattern is detected in the smallest size it can “travel” throughout the layers and its presence detected in the picture. 

. Back to our original question: why should using different sizes during training give better performance than just picking the “proper” size from the beginning? Aren’t convnets indiferent to scale variations? And why not achieve this through image augmentation?

Hopefully after our little thinking on the subject we can attempt to answer now this questions: 

The easiest one, about image augmentation, its main use is to increase the number of images adding variety but if the net is to learn systematically about scale dependent patterns all images should be available to it, and all scaled to the same proportion unlike usual image augmentation is done (with randomnes conditioning both if transformation is made and the proportion of individual transformations).

The important question onw why should this work is clear, I think, after realizing that images contain scale-dependent patterns that a CNN will only learn depending on the combination of size of image and  net architecture. This suggests that ensembling approaches combining image sizes could work. The idea of using the training process to “learn” how to do this ensemble is straightforward and efficient.
